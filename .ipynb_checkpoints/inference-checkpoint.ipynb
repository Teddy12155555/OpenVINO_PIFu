{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "super-occupation",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import tqdm\n",
    "from openvino.runtime import Core, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "lined-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = \"./data\"\n",
    "LOADSIZE = 512\n",
    "NAME = \"pifu_ov\"\n",
    "DEBUG = False\n",
    "NUM_VIEWS = 1\n",
    "RANDOM_MULTIVIEW = False\n",
    "GPU_ID = 0\n",
    "GPU_IDS = 0\n",
    "NUM_THREADS = 1\n",
    "SERIAL_BATCHES = False\n",
    "PIN_MEMORY = False\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "LEARNING_RATEC = 0.001\n",
    "NUM_EPOCH = 100\n",
    "FREQ_PLOT = 10\n",
    "FREQ_SAVE = 50\n",
    "FREQ_SAVE_PLY = 100\n",
    "NO_GEN_MESH = False\n",
    "NO_NUM_EVAL = False\n",
    "RESUME_EPOCH = -1\n",
    "CONTINUE_TRAIN = False\n",
    "RESOLUTION = 256\n",
    "TEST_FOLDER_PATH = \"./input_images\"\n",
    "SIGMA = 5.0\n",
    "NUM_SAMPLE_INOUT = 5000\n",
    "NUM_SAMPLE_COLOR = 0\n",
    "Z_SIZE = 200.0\n",
    "NORM = \"group\"\n",
    "NORM_COLOR = \"group\"\n",
    "NUM_STACK = 4\n",
    "NUM_HOURGLASS = 2\n",
    "SKIP_HOURGLASS = False\n",
    "HG_DOWN = \"ave_pool\"\n",
    "HOURGLASS_DIM = 256\n",
    "MLP_DIM = [257, 1024, 512, 256, 128, 1]\n",
    "MLP_DIM_COLOR = [513, 1024, 512, 256, 128, 3]\n",
    "USE_TANH = False\n",
    "RANDOM_FLIP = False\n",
    "RANDOM_TRANS = False\n",
    "RANDOM_SCALE = False\n",
    "NO_RESIDUAL = False\n",
    "SCHEDULE = [60, 80]\n",
    "GAMMA = 0.1\n",
    "COLOR_LOSS_TYPE = \"l1\"\n",
    "VAL_TEST_ERROR = False\n",
    "VAL_TRAIN_ERROR = False\n",
    "GEN_TEST_MESH = False\n",
    "GEN_TRAIN_MESH = False\n",
    "ALL_MESH = False\n",
    "NUM_GEN_MESH_TEST = 1\n",
    "CHECKPOINTS_PATH = \"./checkpoints\"\n",
    "LOAD_NETG_CHECKPOINT_PATH = \"./checkpoints/net_G\"\n",
    "LOAD_NETC_CHECKPOINT_PATH = \"./checkpoints/net_C\"\n",
    "RESULTS_PATH = \"./results\"\n",
    "LOAD_CHECKPOINT_PATH = None\n",
    "SINGLE = \"\"\n",
    "MASK_PATH = None\n",
    "IMG_PATH = None\n",
    "AUG_ALSTD = 0.0\n",
    "AUG_BRI = 0.0\n",
    "AUG_CON = 0.0\n",
    "AUG_SAT = 0.0\n",
    "AUG_HUE = 0.0\n",
    "AUG_BLUR = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "sonic-tower",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global LOADSIZE\n",
    "def load_image(image_path, mask_path):\n",
    "        self_transforms = transforms.Compose([\n",
    "            transforms.Resize(LOADSIZE),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        # Name\n",
    "        img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        # Calib\n",
    "        B_MIN = np.array([-1, -1, -1])\n",
    "        B_MAX = np.array([1, 1, 1])\n",
    "        projection_matrix = np.identity(4)\n",
    "        projection_matrix[1, 1] = -1\n",
    "        calib = torch.Tensor(projection_matrix).float()\n",
    "        # Mask\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        mask = transforms.Resize(LOADSIZE)(mask)\n",
    "        mask = transforms.ToTensor()(mask).float()\n",
    "        # image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self_transforms(image)\n",
    "        image = mask.expand_as(image) * image\n",
    "        # Return JSON \n",
    "        return {\n",
    "            'name': img_name,\n",
    "            'img': image.unsqueeze(0),\n",
    "            'calib': calib.unsqueeze(0),\n",
    "            'mask': mask.unsqueeze(0),\n",
    "            'b_min': B_MIN,\n",
    "            'b_max': B_MAX,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "vanilla-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj_mesh_with_color(mesh_path, verts, faces, colors):\n",
    "    file = open(mesh_path, 'w')\n",
    "\n",
    "    for idx, v in enumerate(verts):\n",
    "        c = colors[idx]\n",
    "        file.write('v %.4f %.4f %.4f %.4f %.4f %.4f\\n' % (v[0], v[1], v[2], c[0], c[1], c[2]))\n",
    "    for f in faces:\n",
    "        f_plus = f + 1\n",
    "        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[2], f_plus[1]))\n",
    "    file.close()\n",
    "\n",
    "def batch_eval(points, eval_func, num_samples=512 * 512 * 512):\n",
    "    num_pts = points.shape[1]\n",
    "    sdf = np.zeros(num_pts)\n",
    "\n",
    "    num_batches = num_pts // num_samples\n",
    "    for i in range(num_batches):\n",
    "        print('Still work in batch_eval')\n",
    "        sdf[i * num_samples:i * num_samples + num_samples] = eval_func(\n",
    "            points[:, i * num_samples:i * num_samples + num_samples])\n",
    "    if num_pts % num_samples:\n",
    "        sdf[num_batches * num_samples:] = eval_func(points[:, num_batches * num_samples:])\n",
    "\n",
    "    return sdf\n",
    "\n",
    "def eval_grid_octree(coords, eval_func,init_resolution=64, threshold=0.01,num_samples=512 * 512 * 512):\n",
    "    resolution = coords.shape[1:4]\n",
    "\n",
    "    sdf = np.zeros(resolution)\n",
    "\n",
    "    dirty = np.ones(resolution, dtype=np.bool)\n",
    "    grid_mask = np.zeros(resolution, dtype=np.bool)\n",
    "\n",
    "    reso = resolution[0] // init_resolution\n",
    "\n",
    "    while reso > 0:\n",
    "        \n",
    "        # subdivide the grid\n",
    "        grid_mask[0:resolution[0]:reso, 0:resolution[1]:reso, 0:resolution[2]:reso] = True\n",
    "        # test samples in this iteration\n",
    "        test_mask = np.logical_and(grid_mask, dirty)\n",
    "        #print('step size:', reso, 'test sample size:', test_mask.sum())\n",
    "        points = coords[:, test_mask]\n",
    "        \n",
    "        sdf[test_mask] = batch_eval(points, eval_func, num_samples=num_samples)\n",
    "        \n",
    "        \n",
    "        dirty[test_mask] = False\n",
    "        \n",
    "        # do interpolation\n",
    "        if reso <= 1:\n",
    "            break\n",
    "        for x in range(0, resolution[0] - reso, reso):\n",
    "            for y in range(0, resolution[1] - reso, reso):\n",
    "                for z in range(0, resolution[2] - reso, reso):\n",
    "                    # if center marked, return\n",
    "                    if not dirty[x + reso // 2, y + reso // 2, z + reso // 2]:\n",
    "                        continue\n",
    "                    v0 = sdf[x, y, z]\n",
    "                    v1 = sdf[x, y, z + reso]\n",
    "                    v2 = sdf[x, y + reso, z]\n",
    "                    v3 = sdf[x, y + reso, z + reso]\n",
    "                    v4 = sdf[x + reso, y, z]\n",
    "                    v5 = sdf[x + reso, y, z + reso]\n",
    "                    v6 = sdf[x + reso, y + reso, z]\n",
    "                    v7 = sdf[x + reso, y + reso, z + reso]\n",
    "                    v = np.array([v0, v1, v2, v3, v4, v5, v6, v7])\n",
    "                    v_min = v.min()\n",
    "                    v_max = v.max()\n",
    "                    # this cell is all the same\n",
    "                    if (v_max - v_min) < threshold:\n",
    "                        sdf[x:x + reso, y:y + reso, z:z + reso] = (v_max + v_min) / 2\n",
    "                        dirty[x:x + reso, y:y + reso, z:z + reso] = False\n",
    "        reso //= 2\n",
    "\n",
    "    return sdf.reshape(resolution)\n",
    "\n",
    "def create_grid(resX, resY, resZ, b_min=np.array([0, 0, 0]), b_max=np.array([1, 1, 1]), transform=None):\n",
    "    coords = np.mgrid[:resX, :resY, :resZ]\n",
    "    coords = coords.reshape(3, -1)\n",
    "    coords_matrix = np.eye(4)\n",
    "    length = b_max - b_min\n",
    "    coords_matrix[0, 0] = length[0] / resX\n",
    "    coords_matrix[1, 1] = length[1] / resY\n",
    "    coords_matrix[2, 2] = length[2] / resZ\n",
    "    coords_matrix[0:3, 3] = b_min\n",
    "    coords = np.matmul(coords_matrix[:3, :3], coords) + coords_matrix[:3, 3:4]\n",
    "    if transform is not None:\n",
    "        coords = np.matmul(transform[:3, :3], coords) + transform[:3, 3:4]\n",
    "        coords_matrix = np.matmul(transform, coords_matrix)\n",
    "    coords = coords.reshape(3, resX, resY, resZ)\n",
    "    return coords, coords_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "otherwise-share",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(net, cuda, calib_tensor,\n",
    "                   resolution, b_min, b_max,\n",
    "                   use_octree=False, num_samples=10000, transform=None):\n",
    "    coords, mat = create_grid(resolution, resolution, resolution,\n",
    "                              b_min, b_max, transform=transform)\n",
    "\n",
    "    # Then we define the lambda function for cell evaluation\n",
    "    def eval_func(points):\n",
    "        points = np.expand_dims(points, axis=0)\n",
    "        points = np.repeat(points, net.num_views, axis=0)\n",
    "        samples = torch.from_numpy(points).to(device=cuda).float()\n",
    "        print('='*10)\n",
    "\n",
    "        net.query(samples, calib_tensor)\n",
    "        pred = net.get_preds()[0][0]\n",
    "        return pred.detach().cpu().numpy()\n",
    "\n",
    "    # Then we evaluate the grid\n",
    "    \n",
    "    sdf = eval_grid_octree(coords, eval_func, num_samples=num_samples)\n",
    "\n",
    "    # Finally we do marching cubes\n",
    "    try:\n",
    "        verts, faces, normals, values = measure.marching_cubes_lewiner(sdf, 0.5)\n",
    "        # transform verts into world coordinate system\n",
    "        verts = np.matmul(mat[:3, :3], verts.T) + mat[:3, 3:4]\n",
    "        verts = verts.T\n",
    "        return verts, faces, normals, values\n",
    "    except:\n",
    "        print('error cannot marching cubes')\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ambient-fiber",
   "metadata": {},
   "source": [
    "### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "imported-scheduling",
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(feat, uv):\n",
    "    '''\n",
    "    :param feat: [B, C, H, W] image features\n",
    "    :param uv: [B, 2, N] uv coordinates in the image plane, range [-1, 1]\n",
    "    :return: [B, C, N] image features at the uv coordinates\n",
    "    '''\n",
    "    uv = uv.transpose(1, 2)  # [B, N, 2]\n",
    "    uv = uv.unsqueeze(2)  # [B, N, 1, 2]\n",
    "    # NOTE: for newer PyTorch, it seems that training results are degraded due to implementation diff in F.grid_sample\n",
    "    # for old versions, simply remove the aligned_corners argument.\n",
    "    samples = torch.nn.functional.grid_sample(feat, uv, align_corners=True)  # [B, C, N, 1]\n",
    "    return samples[:, :, :, 0]  # [B, C, N]\n",
    "\n",
    "\n",
    "def orthogonal(points, calibrations, transforms=None):\n",
    "    '''\n",
    "    Compute the orthogonal projections of 3D points into the image plane by given projection matrix\n",
    "    :param points: [B, 3, N] Tensor of 3D points\n",
    "    :param calibrations: [B, 4, 4] Tensor of projection matrix\n",
    "    :param transforms: [B, 2, 3] Tensor of image transform matrix\n",
    "    :return: xyz: [B, 3, N] Tensor of xyz coordinates in the image plane\n",
    "    '''\n",
    "    rot = calibrations[:, :3, :3]\n",
    "    trans = calibrations[:, :3, 3:4]\n",
    "    pts = torch.baddbmm(trans, rot, points)  # [B, 3, N]\n",
    "    if transforms is not None:\n",
    "        scale = transforms[:2, :2]\n",
    "        shift = transforms[:2, 2:3]\n",
    "        pts[:, :2, :] = torch.baddbmm(shift, scale, pts[:, :2, :])\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "female-stephen",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGPIFuNet(nn.Module):\n",
    "    def __init__(self,projection_mode='orthogonal',error_term=nn.MSELoss()):\n",
    "        super(HGPIFuNet, self).__init__()\n",
    "        self.name = 'hgpifu'\n",
    "\n",
    "        self.error_term = error_term\n",
    "\n",
    "        self.index = index\n",
    "        self.projection = orthogonal\n",
    "\n",
    "        self.preds = None\n",
    "        self.labels = None\n",
    "\n",
    "        self.num_views = 1\n",
    "\n",
    "        ##############\n",
    "        #  OV field  #\n",
    "        ##############\n",
    "        self.core = Core()\n",
    "        self.device_name = 'CPU'\n",
    "        self.HGF_path = './OV_model/FP16//HGFilter.xml'\n",
    "        self.SC_path = './OV_model/FP16/SurfaceClassifier.xml'\n",
    "        \n",
    "        self.OV_int()\n",
    "        \n",
    "        self.im_feat_list = []\n",
    "        self.tmpx = None\n",
    "        self.normx = None\n",
    "\n",
    "        self.intermediate_preds_list = []\n",
    "\n",
    "    def OV_int(self):\n",
    "        HGF = self.core.read_model(self.HGF_path)\n",
    "        self.HGF = self.core.compile_model(HGF, self.device_name)\n",
    "        \n",
    "        SC = self.core.read_model(self.SC_path)\n",
    "        SC.reshape(\"1, 257, ?\")\n",
    "        self.SC = self.core.compile_model(SC, self.device_name)\n",
    "        \n",
    "        \n",
    "\n",
    "    # replacing normalizer class member to class function.\n",
    "    def normalizer(self, z, calibs=None, index_feat=None):\n",
    "        z_feat = z * (LOADSIZE // 2) / Z_SIZE\n",
    "        return z_feat\n",
    "\n",
    "    \n",
    "    def filter(self, images):\n",
    "        \n",
    "        input_tensor = images.cpu().numpy()\n",
    "        \n",
    "        results = self.HGF.infer_new_request({0: input_tensor})\n",
    "        \n",
    "        for i, (k, v) in enumerate(results.items()):\n",
    "            if i == 3:\n",
    "                temp = torch.tensor(v).to('cuda')\n",
    "            if i == 4:\n",
    "                self.tmpx = torch.tensor(v).to('cuda')\n",
    "            if i == 5:\n",
    "                self.normx = torch.tensor(v).to('cuda')\n",
    "        \n",
    "        # If it is not in training, only produce the last im_feat\n",
    "        if not self.training:\n",
    "            self.im_feat_list = self.im_feat_list = [temp]\n",
    "            \n",
    "            \n",
    "    def query(self, points, calibs, transforms=None, labels=None):\n",
    "        \n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "        \n",
    "        \n",
    "        xyz = self.projection(points, calibs, transforms)\n",
    "        xy = xyz[:, :2, :]\n",
    "        z = xyz[:, 2:3, :]\n",
    "\n",
    "        in_img = (xy[:, 0] >= -1.0) & (xy[:, 0] <= 1.0) & (xy[:, 1] >= -1.0) & (xy[:, 1] <= 1.0)\n",
    "        \n",
    "        print('HERE')\n",
    "        \n",
    "        z_feat = self.normalizer(z, calibs=calibs)\n",
    "\n",
    "        self.intermediate_preds_list = []\n",
    "        infer_request = self.SC.create_infer_request()\n",
    "        \n",
    "       \n",
    "        for im_feat in self.im_feat_list:\n",
    "            # [B, Feat_i + z, N]\n",
    "            point_local_feat_list = [self.index(im_feat, xy), z_feat]\n",
    "\n",
    "            point_local_feat = torch.cat(point_local_feat_list, 1)\n",
    "            \n",
    "            dy_shape = point_local_feat.shape[2]\n",
    "            print(dy_shape)\n",
    "            input_tensor_shape = Tensor(model.input().element_type, [1, 257, dy_shape])\n",
    "            infer_request.set_input_tensor(input_tensor_shape)\n",
    "            input_tensor = point_local_feat.cpu().numpy()\n",
    "            infer_request.infer([input_tensor])\n",
    "            result = infer_request.get_output_tensor()\n",
    "            \n",
    "            pred = in_img[:,None].float() * torch.Tensor(result.data[:]).to('cuda')\n",
    "            self.intermediate_preds_list.append(pred)\n",
    "\n",
    "        self.preds = self.intermediate_preds_list[-1]\n",
    "\n",
    "    def get_im_feat(self):\n",
    "        return self.im_feat_list[-1]\n",
    "\n",
    "    def get_preds(self):\n",
    "        return self.preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "tropical-prayer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mesh(net, cuda, data, save_path, use_octree=True):\n",
    "    image_tensor = data['img'].to(device=cuda)\n",
    "    calib_tensor = data['calib'].to(device=cuda)\n",
    "\n",
    "    net.filter(image_tensor)\n",
    "\n",
    "    b_min = data['b_min']\n",
    "    b_max = data['b_max']\n",
    "    try:\n",
    "        save_img_path = save_path[:-4] + '.png'\n",
    "        save_img_list = []\n",
    "        for v in range(image_tensor.shape[0]):\n",
    "            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n",
    "            save_img_list.append(save_img)\n",
    "        save_img = np.concatenate(save_img_list, axis=1)\n",
    "        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n",
    "\n",
    "        verts, faces, _, _ = reconstruction(\n",
    "            net,\n",
    "            cuda,\n",
    "            calib_tensor,\n",
    "            RESOLUTION,\n",
    "            b_min,\n",
    "            b_max,\n",
    "            use_octree=use_octree\n",
    "        )\n",
    "            \n",
    "        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n",
    "        xyz_tensor = net.projection(verts_tensor, calib_tensor[:1])\n",
    "        uv = xyz_tensor[:, :2, :]\n",
    "        color = index(image_tensor[:1], uv).detach().cpu().numpy()[0].T\n",
    "        color = color * 0.5 + 0.5\n",
    "        save_obj_mesh_with_color(save_path, verts, faces, color)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Can not create marching cubes at this time.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exposed-windows",
   "metadata": {},
   "source": [
    "## Input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "published-gothic",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './input_images\\\\ryota.png'\n",
    "MASK_PATH = './input_images\\\\ryota_mask.png'\n",
    "\n",
    "data = load_image(IMAGE_PATH, MASK_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alert-ability",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "touched-studio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load models\n",
    "netG = HGPIFuNet('orthogonal').to(device='cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "useful-works",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Still work in batch_eval\n",
      "==========\n",
      "HERE ++++++++++++++++++++\n",
      "list index out of range\n",
      "Can not create marching cubes at this time.\n"
     ]
    }
   ],
   "source": [
    "save_path = '%s/%s/result_%s.obj' % (RESULTS_PATH, NAME, data['name'])\n",
    "gen_mesh(netG, 'cuda', data, save_path=save_path, use_octree=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "round-workplace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-celebration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "transparent-fossil",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "constant-inventory",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\micke\\Desktop\\碩論\\OpenVINO_PIFu\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bulgarian-subscription",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
