{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import init\n",
    "# from lib.model.HGPIFuNet import HGPIFuNet\n",
    "# from lib.model.SurfaceClassifier import SurfaceClassifier\n",
    "# from lib.model.DepthNormalizer import DepthNormalizer\n",
    "from torchsummary import summary\n",
    "from torchvision import transforms\n",
    "#from openvino.runtime import (Core)\n",
    "from PIL import Image\n",
    "from skimage import measure\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import json\n",
    "import cv2\n",
    "import os \n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\n"
     ]
    }
   ],
   "source": [
    "! . ~/intel/openvino_2022/setupvars.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[setupvars.sh] OpenVINO environment initialized\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'openvino'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/home/teddy/Desktop/碩論/PIFu/workspace.ipynb Cell 2'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2B140.118.157.8/home/teddy/Desktop/%E7%A2%A9%E8%AB%96/PIFu/workspace.ipynb#ch0000019vscode-remote?line=0'>1</a>\u001b[0m get_ipython()\u001b[39m.\u001b[39msystem(\u001b[39m'\u001b[39m\u001b[39m . ~/intel/openvino_2022/setupvars.sh\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2B140.118.157.8/home/teddy/Desktop/%E7%A2%A9%E8%AB%96/PIFu/workspace.ipynb#ch0000019vscode-remote?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mopenvino\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mruntime\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'openvino'"
     ]
    }
   ],
   "source": [
    "\n",
    "import openvino.runtime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATAROOT = \"./data\"\n",
    "LOADSIZE = 256\n",
    "NAME = \"pifu_ov\"\n",
    "DEBUG = False\n",
    "NUM_VIEWS = 1\n",
    "RANDOM_MULTIVIEW = False\n",
    "GPU_ID = 0\n",
    "GPU_IDS = 0\n",
    "NUM_THREADS = 1\n",
    "SERIAL_BATCHES = False\n",
    "PIN_MEMORY = False\n",
    "BATCH_SIZE = 1\n",
    "LEARNING_RATE = 0.001\n",
    "LEARNING_RATEC = 0.001\n",
    "NUM_EPOCH = 100\n",
    "FREQ_PLOT = 10\n",
    "FREQ_SAVE = 50\n",
    "FREQ_SAVE_PLY = 100\n",
    "NO_GEN_MESH = False\n",
    "NO_NUM_EVAL = False\n",
    "RESUME_EPOCH = -1\n",
    "CONTINUE_TRAIN = False\n",
    "RESOLUTION = 256\n",
    "TEST_FOLDER_PATH = \"./input_images\"\n",
    "SIGMA = 5.0\n",
    "NUM_SAMPLE_INOUT = 5000\n",
    "NUM_SAMPLE_COLOR = 0\n",
    "Z_SIZE = 200.0\n",
    "NORM = \"group\"\n",
    "NORM_COLOR = \"group\"\n",
    "NUM_STACK = 4\n",
    "NUM_HOURGLASS = 2\n",
    "SKIP_HOURGLASS = False\n",
    "HG_DOWN = \"ave_pool\"\n",
    "HOURGLASS_DIM = 256\n",
    "MLP_DIM = [257, 1024, 512, 256, 128, 1]\n",
    "MLP_DIM_COLOR = [513, 1024, 512, 256, 128, 3]\n",
    "USE_TANH = False\n",
    "RANDOM_FLIP = False\n",
    "RANDOM_TRANS = False\n",
    "RANDOM_SCALE = False\n",
    "NO_RESIDUAL = False\n",
    "SCHEDULE = [60, 80]\n",
    "GAMMA = 0.1\n",
    "COLOR_LOSS_TYPE = \"l1\"\n",
    "VAL_TEST_ERROR = False\n",
    "VAL_TRAIN_ERROR = False\n",
    "GEN_TEST_MESH = False\n",
    "GEN_TRAIN_MESH = False\n",
    "ALL_MESH = False\n",
    "NUM_GEN_MESH_TEST = 1\n",
    "CHECKPOINTS_PATH = \"./checkpoints\"\n",
    "LOAD_NETG_CHECKPOINT_PATH = \"./checkpoints/net_G\"\n",
    "LOAD_NETC_CHECKPOINT_PATH = \"./checkpoints/net_C\"\n",
    "RESULTS_PATH = \"./results\"\n",
    "LOAD_CHECKPOINT_PATH = None\n",
    "SINGLE = \"\"\n",
    "MASK_PATH = None\n",
    "IMG_PATH = None\n",
    "AUG_ALSTD = 0.0\n",
    "AUG_BRI = 0.0\n",
    "AUG_CON = 0.0\n",
    "AUG_SAT = 0.0\n",
    "AUG_HUE = 0.0\n",
    "AUG_BLUR = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_eval(points, eval_func, num_samples=512 * 512 * 512):\n",
    "    num_pts = points.shape[1]\n",
    "    sdf = np.zeros(num_pts)\n",
    "\n",
    "    num_batches = num_pts // num_samples\n",
    "    for i in range(num_batches):\n",
    "        sdf[i * num_samples:i * num_samples + num_samples] = eval_func(\n",
    "            points[:, i * num_samples:i * num_samples + num_samples])\n",
    "    if num_pts % num_samples:\n",
    "        sdf[num_batches * num_samples:] = eval_func(points[:, num_batches * num_samples:])\n",
    "\n",
    "    return sdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_grid_octree(coords, eval_func,init_resolution=64, threshold=0.01,num_samples=512 * 512 * 512):\n",
    "    resolution = coords.shape[1:4]\n",
    "\n",
    "    sdf = np.zeros(resolution)\n",
    "\n",
    "    dirty = np.ones(resolution, dtype=np.bool)\n",
    "    grid_mask = np.zeros(resolution, dtype=np.bool)\n",
    "\n",
    "    reso = resolution[0] // init_resolution\n",
    "\n",
    "    while reso > 0:\n",
    "        # subdivide the grid\n",
    "        grid_mask[0:resolution[0]:reso, 0:resolution[1]:reso, 0:resolution[2]:reso] = True\n",
    "        # test samples in this iteration\n",
    "        test_mask = np.logical_and(grid_mask, dirty)\n",
    "        #print('step size:', reso, 'test sample size:', test_mask.sum())\n",
    "        points = coords[:, test_mask]\n",
    "\n",
    "        sdf[test_mask] = batch_eval(points, eval_func, num_samples=num_samples)\n",
    "        dirty[test_mask] = False\n",
    "\n",
    "        # do interpolation\n",
    "        if reso <= 1:\n",
    "            break\n",
    "        for x in range(0, resolution[0] - reso, reso):\n",
    "            for y in range(0, resolution[1] - reso, reso):\n",
    "                for z in range(0, resolution[2] - reso, reso):\n",
    "                    # if center marked, return\n",
    "                    if not dirty[x + reso // 2, y + reso // 2, z + reso // 2]:\n",
    "                        continue\n",
    "                    v0 = sdf[x, y, z]\n",
    "                    v1 = sdf[x, y, z + reso]\n",
    "                    v2 = sdf[x, y + reso, z]\n",
    "                    v3 = sdf[x, y + reso, z + reso]\n",
    "                    v4 = sdf[x + reso, y, z]\n",
    "                    v5 = sdf[x + reso, y, z + reso]\n",
    "                    v6 = sdf[x + reso, y + reso, z]\n",
    "                    v7 = sdf[x + reso, y + reso, z + reso]\n",
    "                    v = np.array([v0, v1, v2, v3, v4, v5, v6, v7])\n",
    "                    v_min = v.min()\n",
    "                    v_max = v.max()\n",
    "                    # this cell is all the same\n",
    "                    if (v_max - v_min) < threshold:\n",
    "                        sdf[x:x + reso, y:y + reso, z:z + reso] = (v_max + v_min) / 2\n",
    "                        dirty[x:x + reso, y:y + reso, z:z + reso] = False\n",
    "        reso //= 2\n",
    "\n",
    "    return sdf.reshape(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_grid(coords, eval_func, num_samples=512 * 512 * 512):\n",
    "    resolution = coords.shape[1:4]\n",
    "    coords = coords.reshape([3, -1])\n",
    "    sdf = batch_eval(coords, eval_func, num_samples=num_samples)\n",
    "    return sdf.reshape(resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_grid(resX, resY, resZ, b_min=np.array([0, 0, 0]), b_max=np.array([1, 1, 1]), transform=None):\n",
    "    '''\n",
    "    Create a dense grid of given resolution and bounding box\n",
    "    :param resX: resolution along X axis\n",
    "    :param resY: resolution along Y axis\n",
    "    :param resZ: resolution along Z axis\n",
    "    :param b_min: vec3 (x_min, y_min, z_min) bounding box corner\n",
    "    :param b_max: vec3 (x_max, y_max, z_max) bounding box corner\n",
    "    :return: [3, resX, resY, resZ] coordinates of the grid, and transform matrix from mesh index\n",
    "    '''\n",
    "    coords = np.mgrid[:resX, :resY, :resZ]\n",
    "    coords = coords.reshape(3, -1)\n",
    "    coords_matrix = np.eye(4)\n",
    "    length = b_max - b_min\n",
    "    coords_matrix[0, 0] = length[0] / resX\n",
    "    coords_matrix[1, 1] = length[1] / resY\n",
    "    coords_matrix[2, 2] = length[2] / resZ\n",
    "    coords_matrix[0:3, 3] = b_min\n",
    "    coords = np.matmul(coords_matrix[:3, :3], coords) + coords_matrix[:3, 3:4]\n",
    "    if transform is not None:\n",
    "        coords = np.matmul(transform[:3, :3], coords) + transform[:3, 3:4]\n",
    "        coords_matrix = np.matmul(transform, coords_matrix)\n",
    "    coords = coords.reshape(3, resX, resY, resZ)\n",
    "    return coords, coords_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruction(net, cuda, calib_tensor,\n",
    "                   resolution, b_min, b_max,\n",
    "                   use_octree=False, num_samples=10000, transform=None):\n",
    "    '''\n",
    "    Reconstruct meshes from sdf predicted by the network.\n",
    "    :param net: a BasePixImpNet object. call image filter beforehead.\n",
    "    :param cuda: cuda device\n",
    "    :param calib_tensor: calibration tensor\n",
    "    :param resolution: resolution of the grid cell\n",
    "    :param b_min: bounding box corner [x_min, y_min, z_min]\n",
    "    :param b_max: bounding box corner [x_max, y_max, z_max]\n",
    "    :param use_octree: whether to use octree acceleration\n",
    "    :param num_samples: how many points to query each gpu iteration\n",
    "    :return: marching cubes results.\n",
    "    '''\n",
    "    # First we create a grid by resolution\n",
    "    # and transforming matrix for grid coordinates to real world xyz\n",
    "    coords, mat = create_grid(resolution, resolution, resolution,\n",
    "                              b_min, b_max, transform=transform)\n",
    "\n",
    "    # Then we define the lambda function for cell evaluation\n",
    "    def eval_func(points):\n",
    "        points = np.expand_dims(points, axis=0)\n",
    "        points = np.repeat(points, net.num_views, axis=0)\n",
    "        samples = torch.from_numpy(points).to(device=cuda).float()\n",
    "        net.query(samples, calib_tensor)\n",
    "        pred = net.get_preds()[0][0]\n",
    "        return pred.detach().cpu().numpy()\n",
    "\n",
    "    # Then we evaluate the grid\n",
    "    if use_octree:\n",
    "        sdf = eval_grid_octree(coords, eval_func, num_samples=num_samples)\n",
    "    else:\n",
    "        sdf = eval_grid(coords, eval_func, num_samples=num_samples)\n",
    "\n",
    "    # Finally we do marching cubes\n",
    "    try:\n",
    "        verts, faces, normals, values = measure.marching_cubes_lewiner(sdf, 0.5)\n",
    "        # transform verts into world coordinate system\n",
    "        verts = np.matmul(mat[:3, :3], verts.T) + mat[:3, 3:4]\n",
    "        verts = verts.T\n",
    "        return verts, faces, normals, values\n",
    "    except:\n",
    "        print('error cannot marching cubes')\n",
    "        return -1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj_mesh_with_color(mesh_path, verts, faces, colors):\n",
    "    file = open(mesh_path, 'w')\n",
    "\n",
    "    for idx, v in enumerate(verts):\n",
    "        c = colors[idx]\n",
    "        file.write('v %.4f %.4f %.4f %.4f %.4f %.4f\\n' % (v[0], v[1], v[2], c[0], c[1], c[2]))\n",
    "    for f in faces:\n",
    "        f_plus = f + 1\n",
    "        file.write('f %d %d %d\\n' % (f_plus[0], f_plus[2], f_plus[1]))\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_mesh(net, cuda, data, save_path, use_octree=True):\n",
    "    image_tensor = data['img'].to(device=cuda)\n",
    "    calib_tensor = data['calib'].to(device=cuda)\n",
    "\n",
    "    net.filter(image_tensor)\n",
    "\n",
    "    b_min = data['b_min']\n",
    "    b_max = data['b_max']\n",
    "    try:\n",
    "        save_img_path = save_path[:-4] + '.png'\n",
    "        save_img_list = []\n",
    "        for v in range(image_tensor.shape[0]):\n",
    "            save_img = (np.transpose(image_tensor[v].detach().cpu().numpy(), (1, 2, 0)) * 0.5 + 0.5)[:, :, ::-1] * 255.0\n",
    "            save_img_list.append(save_img)\n",
    "        save_img = np.concatenate(save_img_list, axis=1)\n",
    "        Image.fromarray(np.uint8(save_img[:,:,::-1])).save(save_img_path)\n",
    "\n",
    "        verts, faces, _, _ = reconstruction(\n",
    "            net,\n",
    "            cuda,\n",
    "            calib_tensor,\n",
    "            RESOLUTION,\n",
    "            b_min,\n",
    "            b_max,\n",
    "            use_octree=use_octree\n",
    "        )\n",
    "            \n",
    "        verts_tensor = torch.from_numpy(verts.T).unsqueeze(0).to(device=cuda).float()\n",
    "        xyz_tensor = net.projection(verts_tensor, calib_tensor[:1])\n",
    "        uv = xyz_tensor[:, :2, :]\n",
    "        color = index(image_tensor[:1], uv).detach().cpu().numpy()[0].T\n",
    "        color = color * 0.5 + 0.5\n",
    "        save_obj_mesh_with_color(save_path, verts, faces, color)\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        print('Can not create marching cubes at this time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_net(net, init_type='normal', init_gain=0.02, gpu_ids=[]):\n",
    "    \"\"\"Initialize a network: 1. register CPU/GPU device (with multi-GPU support); 2. initialize the network weights\n",
    "    Parameters:\n",
    "        net (network)      -- the network to be initialized\n",
    "        init_type (str)    -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        gain (float)       -- scaling factor for normal, xavier and orthogonal.\n",
    "        gpu_ids (int list) -- which GPUs the network runs on: e.g., 0,1,2\n",
    "\n",
    "    Return an initialized network.\n",
    "    \"\"\"\n",
    "    if len(gpu_ids) > 0:\n",
    "        assert (torch.cuda.is_available())\n",
    "        net.to(gpu_ids[0])\n",
    "        net = torch.nn.DataParallel(net, gpu_ids)  # multi-GPUs\n",
    "    init_weights(net, init_type, init_gain=init_gain)\n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_weights(net, init_type='normal', init_gain=0.02):\n",
    "    \"\"\"Initialize network weights.\n",
    "\n",
    "    Parameters:\n",
    "        net (network)   -- network to be initialized\n",
    "        init_type (str) -- the name of an initialization method: normal | xavier | kaiming | orthogonal\n",
    "        init_gain (float)    -- scaling factor for normal, xavier and orthogonal.\n",
    "\n",
    "    We use 'normal' in the original pix2pix and CycleGAN paper. But xavier and kaiming might\n",
    "    work better for some applications. Feel free to try yourself.\n",
    "    \"\"\"\n",
    "\n",
    "    def init_func(m):  # define the initialization function\n",
    "        classname = m.__class__.__name__\n",
    "        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n",
    "            if init_type == 'normal':\n",
    "                init.normal_(m.weight.data, 0.0, init_gain)\n",
    "            elif init_type == 'xavier':\n",
    "                init.xavier_normal_(m.weight.data, gain=init_gain)\n",
    "            elif init_type == 'kaiming':\n",
    "                init.kaiming_normal_(m.weight.data, a=0, mode='fan_in')\n",
    "            elif init_type == 'orthogonal':\n",
    "                init.orthogonal_(m.weight.data, gain=init_gain)\n",
    "            else:\n",
    "                raise NotImplementedError('initialization method [%s] is not implemented' % init_type)\n",
    "            if hasattr(m, 'bias') and m.bias is not None:\n",
    "                init.constant_(m.bias.data, 0.0)\n",
    "        elif classname.find(\n",
    "                'BatchNorm2d') != -1:  # BatchNorm Layer's weight is not a matrix; only normal distribution applies.\n",
    "            init.normal_(m.weight.data, 1.0, init_gain)\n",
    "            init.constant_(m.bias.data, 0.0)\n",
    "\n",
    "    print('initialize network with %s' % init_type)\n",
    "    net.apply(init_func)  # apply the initialization function <init_func>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index(feat, uv):\n",
    "    '''\n",
    "    :param feat: [B, C, H, W] image features\n",
    "    :param uv: [B, 2, N] uv coordinates in the image plane, range [-1, 1]\n",
    "    :return: [B, C, N] image features at the uv coordinates\n",
    "    '''\n",
    "    uv = uv.transpose(1, 2)  # [B, N, 2]\n",
    "    uv = uv.unsqueeze(2)  # [B, N, 1, 2]\n",
    "    # NOTE: for newer PyTorch, it seems that training results are degraded due to implementation diff in F.grid_sample\n",
    "    # for old versions, simply remove the aligned_corners argument.\n",
    "    samples = torch.nn.functional.grid_sample(feat, uv, align_corners=True)  # [B, C, N, 1]\n",
    "    return samples[:, :, :, 0]  # [B, C, N]\n",
    "\n",
    "\n",
    "def orthogonal(points, calibrations, transforms=None):\n",
    "    '''\n",
    "    Compute the orthogonal projections of 3D points into the image plane by given projection matrix\n",
    "    :param points: [B, 3, N] Tensor of 3D points\n",
    "    :param calibrations: [B, 4, 4] Tensor of projection matrix\n",
    "    :param transforms: [B, 2, 3] Tensor of image transform matrix\n",
    "    :return: xyz: [B, 3, N] Tensor of xyz coordinates in the image plane\n",
    "    '''\n",
    "    rot = calibrations[:, :3, :3]\n",
    "    trans = calibrations[:, :3, 3:4]\n",
    "    pts = torch.baddbmm(trans, rot, points)  # [B, 3, N]\n",
    "    if transforms is not None:\n",
    "        scale = transforms[:2, :2]\n",
    "        shift = transforms[:2, 2:3]\n",
    "        pts[:, :2, :] = torch.baddbmm(shift, scale, pts[:, :2, :])\n",
    "    return pts\n",
    "\n",
    "\n",
    "def perspective(points, calibrations, transforms=None):\n",
    "    '''\n",
    "    Compute the perspective projections of 3D points into the image plane by given projection matrix\n",
    "    :param points: [Bx3xN] Tensor of 3D points\n",
    "    :param calibrations: [Bx4x4] Tensor of projection matrix\n",
    "    :param transforms: [Bx2x3] Tensor of image transform matrix\n",
    "    :return: xy: [Bx2xN] Tensor of xy coordinates in the image plane\n",
    "    '''\n",
    "    rot = calibrations[:, :3, :3]\n",
    "    trans = calibrations[:, :3, 3:4]\n",
    "    homo = torch.baddbmm(trans, rot, points)  # [B, 3, N]\n",
    "    xy = homo[:, :2, :] / homo[:, 2:3, :]\n",
    "    if transforms is not None:\n",
    "        scale = transforms[:2, :2]\n",
    "        shift = transforms[:2, 2:3]\n",
    "        xy = torch.baddbmm(shift, scale, xy)\n",
    "\n",
    "    xyz = torch.cat([xy, homo[:, 2:3, :]], 1)\n",
    "    return xyz\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv3x3(in_planes, out_planes, strd=1, padding=1, bias=False):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3,\n",
    "                     stride=strd, padding=padding, bias=bias)\n",
    "\n",
    "# class Flatten(nn.Module):\n",
    "#     def forward(self, input):\n",
    "#         return input.view(input.size(0), -1)\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, norm='batch'):\n",
    "        super(ConvBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(in_planes, int(out_planes / 2))\n",
    "        self.conv2 = conv3x3(int(out_planes / 2), int(out_planes / 4))\n",
    "        self.conv3 = conv3x3(int(out_planes / 4), int(out_planes / 4))\n",
    "\n",
    "        if norm == 'batch':\n",
    "            self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "            self.bn2 = nn.BatchNorm2d(int(out_planes / 2))\n",
    "            self.bn3 = nn.BatchNorm2d(int(out_planes / 4))\n",
    "            self.bn4 = nn.BatchNorm2d(in_planes)\n",
    "        elif norm == 'group':\n",
    "            self.bn1 = nn.GroupNorm(32, in_planes)\n",
    "            self.bn2 = nn.GroupNorm(32, int(out_planes / 2))\n",
    "            self.bn3 = nn.GroupNorm(32, int(out_planes / 4))\n",
    "            self.bn4 = nn.GroupNorm(32, in_planes)\n",
    "        \n",
    "        if in_planes != out_planes:\n",
    "            self.downsample = nn.Sequential(\n",
    "                self.bn4,\n",
    "                nn.ReLU(True),\n",
    "                nn.Conv2d(in_planes, out_planes,\n",
    "                          kernel_size=1, stride=1, bias=False),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out1 = self.bn1(x)\n",
    "        out1 = F.relu(out1, True)\n",
    "        out1 = self.conv1(out1)\n",
    "\n",
    "        out2 = self.bn2(out1)\n",
    "        out2 = F.relu(out2, True)\n",
    "        out2 = self.conv2(out2)\n",
    "\n",
    "        out3 = self.bn3(out2)\n",
    "        out3 = F.relu(out3, True)\n",
    "        out3 = self.conv3(out3)\n",
    "\n",
    "        out3 = torch.cat((out1, out2, out3), 1)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(residual)\n",
    "\n",
    "        out3 += residual\n",
    "\n",
    "        return out3\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HourGlass(nn.Module):\n",
    "    def __init__(self, num_modules, depth, num_features, norm='batch'):\n",
    "        super(HourGlass, self).__init__()\n",
    "        self.num_modules = num_modules\n",
    "        self.depth = depth\n",
    "        self.features = num_features\n",
    "        self.norm = norm\n",
    "\n",
    "        self._generate_network(self.depth)\n",
    "\n",
    "    def _generate_network(self, level):\n",
    "        self.add_module('b1_' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n",
    "\n",
    "        self.add_module('b2_' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n",
    "\n",
    "        if level > 1:\n",
    "            self._generate_network(level - 1)\n",
    "        else:\n",
    "            self.add_module('b2_plus_' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n",
    "\n",
    "        self.add_module('b3_' + str(level), ConvBlock(self.features, self.features, norm=self.norm))\n",
    "\n",
    "    def _forward(self, level, inp):\n",
    "        # Upper branch\n",
    "        up1 = inp\n",
    "        up1 = self._modules['b1_' + str(level)](up1)\n",
    "\n",
    "        # Lower branch\n",
    "        low1 = F.avg_pool2d(inp, 2, stride=2)\n",
    "        low1 = self._modules['b2_' + str(level)](low1)\n",
    "\n",
    "        if level > 1:\n",
    "            low2 = self._forward(level - 1, low1)\n",
    "        else:\n",
    "            low2 = low1\n",
    "            low2 = self._modules['b2_plus_' + str(level)](low2)\n",
    "\n",
    "        low3 = low2\n",
    "        low3 = self._modules['b3_' + str(level)](low3)\n",
    "\n",
    "        # NOTE: for newer PyTorch (1.3~), it seems that training results are degraded due to implementation diff in F.grid_sample\n",
    "        # if the pretrained model behaves weirdly, switch with the commented line.\n",
    "        # NOTE: I also found that \"bicubic\" works better.\n",
    "        up2 = F.interpolate(low3, scale_factor=2, mode='bicubic', align_corners=True)\n",
    "        # up2 = F.interpolate(low3, scale_factor=2, mode='nearest)\n",
    "\n",
    "        return up1 + up2\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward(self.depth, x)\n",
    "\n",
    "# class HGFilter(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HGFilter, self).__init__()\n",
    "        self.num_modules = NUM_STACK\n",
    "\n",
    "        # Base part\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)\n",
    "\n",
    "        if NORM == 'batch':\n",
    "            self.bn1 = nn.BatchNorm2d(64)\n",
    "        elif NORM == 'group':\n",
    "            self.bn1 = nn.GroupNorm(32, 64)\n",
    "\n",
    "        if HG_DOWN == 'conv64':\n",
    "            self.conv2 = ConvBlock(64, 64, NORM)\n",
    "            self.down_conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "        elif HG_DOWN == 'conv128':\n",
    "            self.conv2 = ConvBlock(64, 128, NORM)\n",
    "            self.down_conv2 = nn.Conv2d(128, 128, kernel_size=3, stride=2, padding=1)\n",
    "        elif HG_DOWN == 'ave_pool':\n",
    "            self.conv2 = ConvBlock(64, 128, NORM)\n",
    "        else:\n",
    "            raise NameError('Unknown Fan Filter setting!')\n",
    "\n",
    "        self.conv3 = ConvBlock(128, 128, NORM)\n",
    "        self.conv4 = ConvBlock(128, 256, NORM)\n",
    "\n",
    "        # Stacking part\n",
    "        for hg_module in range(self.num_modules):\n",
    "            self.add_module('m' + str(hg_module), HourGlass(1, NUM_HOURGLASS, 256, NORM))\n",
    "\n",
    "            self.add_module('top_m_' + str(hg_module), ConvBlock(256, 256, NORM))\n",
    "            self.add_module('conv_last' + str(hg_module),\n",
    "                            nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
    "            if NORM == 'batch':\n",
    "                self.add_module('bn_end' + str(hg_module), nn.BatchNorm2d(256))\n",
    "            elif NORM == 'group':\n",
    "                self.add_module('bn_end' + str(hg_module), nn.GroupNorm(32, 256))\n",
    "                \n",
    "            self.add_module('l' + str(hg_module), nn.Conv2d(256,\n",
    "                                                            HOURGLASS_DIM, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "            if hg_module < self.num_modules - 1:\n",
    "                self.add_module(\n",
    "                    'bl' + str(hg_module), nn.Conv2d(256, 256, kernel_size=1, stride=1, padding=0))\n",
    "                self.add_module('al' + str(hg_module), nn.Conv2d(HOURGLASS_DIM,\n",
    "                                                                 256, kernel_size=1, stride=1, padding=0))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.conv1(x)), True)\n",
    "        tmpx = x\n",
    "        if HG_DOWN == 'ave_pool':\n",
    "            x = F.avg_pool2d(self.conv2(x), 2, stride=2)\n",
    "        elif HG_DOWN in ['conv64', 'conv128']:\n",
    "            x = self.conv2(x)\n",
    "            x = self.down_conv2(x)\n",
    "        else:\n",
    "            raise NameError('Unknown Fan Filter setting!')\n",
    "\n",
    "        normx = x\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "\n",
    "        previous = x\n",
    "\n",
    "        outputs = []\n",
    "        for i in range(self.num_modules):\n",
    "            hg = self._modules['m' + str(i)](previous)\n",
    "\n",
    "            ll = hg\n",
    "            ll = self._modules['top_m_' + str(i)](ll)\n",
    "\n",
    "            ll = F.relu(self._modules['bn_end' + str(i)]\n",
    "                        (self._modules['conv_last' + str(i)](ll)), True)\n",
    "\n",
    "            # Predict heatmaps\n",
    "            tmp_out = self._modules['l' + str(i)](ll)\n",
    "            outputs.append(tmp_out)\n",
    "\n",
    "            if i < self.num_modules - 1:\n",
    "                ll = self._modules['bl' + str(i)](ll)\n",
    "                tmp_out_ = self._modules['al' + str(i)](tmp_out)\n",
    "                previous = previous + ll + tmp_out_\n",
    "\n",
    "        return outputs, tmpx.detach(), normx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-16-59fc4effa6d2>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-16-59fc4effa6d2>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    def __init__(self, filter_channels, num_views=1, no_residual=True, last_op=None):\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "# class SurfaceClassifier(nn.Module):\n",
    "    def __init__(self, filter_channels, num_views=1, no_residual=True, last_op=None):\n",
    "        super(SurfaceClassifier, self).__init__()\n",
    "\n",
    "        self.filters = []\n",
    "        self.num_views = num_views\n",
    "        self.no_residual = no_residual\n",
    "        filter_channels = filter_channels\n",
    "        self.last_op = last_op\n",
    "\n",
    "        if self.no_residual:\n",
    "            for l in range(0, len(filter_channels) - 1):\n",
    "                self.filters.append(nn.Conv1d(\n",
    "                    filter_channels[l],\n",
    "                    filter_channels[l + 1],\n",
    "                    1))\n",
    "                self.add_module(\"conv%d\" % l, self.filters[l])\n",
    "        else:\n",
    "            for l in range(0, len(filter_channels) - 1):\n",
    "                if 0 != l:\n",
    "                    self.filters.append(\n",
    "                        nn.Conv1d(\n",
    "                            filter_channels[l] + filter_channels[0],\n",
    "                            filter_channels[l + 1],\n",
    "                            1))\n",
    "                else:\n",
    "                    self.filters.append(nn.Conv1d(\n",
    "                        filter_channels[l],\n",
    "                        filter_channels[l + 1],\n",
    "                        1))\n",
    "\n",
    "                self.add_module(\"conv%d\" % l, self.filters[l])\n",
    "\n",
    "    def forward(self, feature):\n",
    "        '''\n",
    "\n",
    "        :param feature: list of [BxC_inxHxW] tensors of image features\n",
    "        :param xy: [Bx3xN] tensor of (x,y) coodinates in the image plane\n",
    "        :return: [BxC_outxN] tensor of features extracted at the coordinates\n",
    "        '''\n",
    "\n",
    "        y = feature\n",
    "        tmpy = feature\n",
    "        for i, f in enumerate(self.filters):\n",
    "            if self.no_residual:\n",
    "                y = self._modules['conv' + str(i)](y)\n",
    "            else:\n",
    "                y = self._modules['conv' + str(i)](\n",
    "                    y if i == 0\n",
    "                    else torch.cat([y, tmpy], 1)\n",
    "                )\n",
    "            if i != len(self.filters) - 1:\n",
    "                y = F.leaky_relu(y)\n",
    "\n",
    "            if self.num_views > 1 and i == len(self.filters) // 2:\n",
    "                y = y.view(\n",
    "                    -1, self.num_views, y.shape[1], y.shape[2]\n",
    "                ).mean(dim=1)\n",
    "                tmpy = feature.view(\n",
    "                    -1, self.num_views, feature.shape[1], feature.shape[2]\n",
    "                ).mean(dim=1)\n",
    "\n",
    "        if self.last_op:\n",
    "            y = self.last_op(y)\n",
    "\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HGPIFuNet(nn.Module):\n",
    "    '''\n",
    "    HG PIFu network uses Hourglass stacks as the image filter.\n",
    "    It does the following:\n",
    "        1. Compute image feature stacks and store it in self.im_feat_list\n",
    "            self.im_feat_list[-1] is the last stack (output stack)\n",
    "        2. Calculate calibration\n",
    "        3. If training, it index on every intermediate stacks,\n",
    "            If testing, it index on the last stack.\n",
    "        4. Classification.\n",
    "        5. During training, error is calculated on all stacks.\n",
    "    '''\n",
    "    def __init__(self,projection_mode='orthogonal',error_term=nn.MSELoss()):\n",
    "        super(HGPIFuNet, self).__init__()\n",
    "        self.name = 'hgpifu'\n",
    "\n",
    "        self.error_term = error_term\n",
    "\n",
    "        self.index = index\n",
    "        self.projection = orthogonal if projection_mode == 'orthogonal' else perspective\n",
    "\n",
    "        self.preds = None\n",
    "        self.labels = None\n",
    "\n",
    "        self.num_views = NUM_VIEWS\n",
    "\n",
    "        vino_core = Core()\n",
    "        # Replace with OpenVINO model\n",
    "        # self.image_filter = HGFilter()\n",
    "        self.image_filter = vino_core.compile_model(vino_core.read_model('./OV_model/FP16//HGFilter.xml'),\"CPU\")\n",
    "        \n",
    "\n",
    "        # Replace with OpenVINO model\n",
    "        self.surface_classifier = SurfaceClassifier(\n",
    "            filter_channels=MLP_DIM,\n",
    "            num_views=NUM_VIEWS,\n",
    "            no_residual=NO_RESIDUAL,\n",
    "            last_op=nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # This is a list of [B x Feat_i x H x W] features\n",
    "        self.im_feat_list = []\n",
    "        self.tmpx = None\n",
    "        self.normx = None\n",
    "\n",
    "        self.intermediate_preds_list = []\n",
    "        \n",
    "        # init_net(self)\n",
    "\n",
    "    # replacing normalizer class member to class function.\n",
    "    def normalizer(self, z, calibs=None, index_feat=None):\n",
    "        z_feat = z * (LOADSIZE // 2) / Z_SIZE\n",
    "        return z_feat\n",
    "\n",
    "    def filter(self, images):\n",
    "        '''\n",
    "        Filter the input images\n",
    "        store all intermediate features.\n",
    "        :param images: [B, C, H, W] input images\n",
    "        '''\n",
    "        #print('DEBUG : ', images.shape)\n",
    "        _, _, _, self.im_feat_list, self.tmpx, self.normx = self.image_filter.infer_new_request({0:images.cpu().numpy()})\n",
    "        self.im_feat_list = torch.tensor( self.im_feat_list).to(\"cuda\")\n",
    "        self.tmpx = torch.tensor( self.tmpx).to(\"cuda\")\n",
    "        self.normx = torch.tensor(self.normx).to(\"cuda\")\n",
    "        \n",
    "        # If it is not in training, only produce the last im_feat\n",
    "        if not self.training:\n",
    "            self.im_feat_list = [self.im_feat_list[-1]]\n",
    "\n",
    "    def query(self, points, calibs, transforms=None, labels=None):\n",
    "        '''\n",
    "        Given 3D points, query the network predictions for each point.\n",
    "        Image features should be pre-computed before this call.\n",
    "        store all intermediate features.\n",
    "        query() function may behave differently during training/testing.\n",
    "        :param points: [B, 3, N] world space coordinates of points\n",
    "        :param calibs: [B, 3, 4] calibration matrices for each image\n",
    "        :param transforms: Optional [B, 2, 3] image space coordinate transforms\n",
    "        :param labels: Optional [B, Res, N] gt labeling\n",
    "        :return: [B, Res, N] predictions for each point\n",
    "        '''\n",
    "        \n",
    "\n",
    "        if labels is not None:\n",
    "            self.labels = labels\n",
    "\n",
    "        xyz = self.projection(points, calibs, transforms)\n",
    "        xy = xyz[:, :2, :]\n",
    "        z = xyz[:, 2:3, :]\n",
    "\n",
    "        \n",
    "\n",
    "        in_img = (xy[:, 0] >= -1.0) & (xy[:, 0] <= 1.0) & (xy[:, 1] >= -1.0) & (xy[:, 1] <= 1.0)\n",
    "\n",
    "        z_feat = self.normalizer(z, calibs=calibs)\n",
    "\n",
    "        if SKIP_HOURGLASS :\n",
    "            tmpx_local_feature = self.index(self.tmpx, xy)\n",
    "\n",
    "        self.intermediate_preds_list = []\n",
    "\n",
    "        for im_feat in self.im_feat_list:\n",
    "            # [B, Feat_i + z, N]\n",
    "            point_local_feat_list = [self.index(im_feat, xy), z_feat]\n",
    "\n",
    "            if SKIP_HOURGLASS:\n",
    "                point_local_feat_list.append(tmpx_local_feature)\n",
    "\n",
    "            point_local_feat = torch.cat(point_local_feat_list, 1)\n",
    "\n",
    "            pred = in_img[:,None].float() * self.surface_classifier(point_local_feat)\n",
    "            self.intermediate_preds_list.append(pred)\n",
    "\n",
    "        self.preds = self.intermediate_preds_list[-1]\n",
    "\n",
    "    def get_im_feat(self):\n",
    "        '''\n",
    "        Get the image filter\n",
    "        :return: [B, C_feat, H, W] image feature after filtering\n",
    "        '''\n",
    "        return self.im_feat_list[-1]\n",
    "\n",
    "    def get_preds(self):\n",
    "        '''\n",
    "        Get the predictions from the last query\n",
    "        :return: [B, Res, N] network prediction for the last query\n",
    "        '''\n",
    "        return self.preds\n",
    "\n",
    "    def get_error(self):\n",
    "        '''\n",
    "        Hourglass has its own intermediate supervision scheme\n",
    "        '''\n",
    "        error = 0\n",
    "        for preds in self.intermediate_preds_list:\n",
    "            error += self.error_term(preds, self.labels)\n",
    "        error /= len(self.intermediate_preds_list)\n",
    "        \n",
    "        return error\n",
    "\n",
    "    def forward(self, images, points, calibs, transforms=None, labels=None):\n",
    "        # Get image feature\n",
    "        self.filter(images)\n",
    "\n",
    "        # Phase 2: point query\n",
    "        self.query(points=points, calibs=calibs, transforms=transforms, labels=labels)\n",
    "\n",
    "        # get the prediction\n",
    "        res = self.get_preds()\n",
    "        \n",
    "        # get the error\n",
    "        error = self.get_error()\n",
    "\n",
    "        return res, error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Evaluator:\n",
    "    def __init__(self,  projection_mode='orthogonal'):\n",
    "        self.load_size = LOADSIZE\n",
    "        self.transforms = transforms.Compose([\n",
    "            transforms.Resize(self.load_size),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "        ])\n",
    "        # set cuda\n",
    "        # cuda = torch.device('cpu')\n",
    "        cuda = torch.device('cuda:0')\n",
    "\n",
    "        # create net\n",
    "        netG = HGPIFuNet(projection_mode).to(device=cuda)\n",
    "        print('Using Network: ', netG.name)\n",
    "\n",
    "       \n",
    "        netG.load_state_dict(torch.load(LOAD_NETG_CHECKPOINT_PATH, map_location=cuda))\n",
    "\n",
    "\n",
    "        os.makedirs(RESULTS_PATH, exist_ok=True)\n",
    "        os.makedirs('%s/%s' % (RESULTS_PATH, NAME), exist_ok=True)\n",
    "\n",
    "        self.cuda = cuda\n",
    "        self.netG = netG\n",
    "        \n",
    "    def to_tensor(self,x):\n",
    "        return self.transforms(x)\n",
    "\n",
    "    def load_image(self, image_path, mask_path):\n",
    "        \"\"\"\n",
    "        這裡把 image 跟 mask 都讀進來，還有一些相機參數，最後把照片乘上 mask 這樣就只會有人的圖像而\n",
    "        不會有背景\n",
    "        \"\"\"\n",
    "        # Name\n",
    "        img_name = os.path.splitext(os.path.basename(image_path))[0]\n",
    "        # Calib\n",
    "        B_MIN = np.array([-1, -1, -1])\n",
    "        B_MAX = np.array([1, 1, 1])\n",
    "        projection_matrix = np.identity(4)\n",
    "        projection_matrix[1, 1] = -1\n",
    "        calib = torch.Tensor(projection_matrix).float()\n",
    "        # Mask\n",
    "        mask = Image.open(mask_path).convert('L')\n",
    "        mask = transforms.Resize(self.load_size)(mask)\n",
    "        mask = transforms.ToTensor()(mask).float()\n",
    "        # image\n",
    "        image = Image.open(image_path).convert('RGB')\n",
    "        image = self.to_tensor(image)\n",
    "        image = mask.expand_as(image) * image\n",
    "        # Return JSON \n",
    "        return {\n",
    "            'name': img_name,\n",
    "            'img': image.unsqueeze(0),\n",
    "            'calib': calib.unsqueeze(0),\n",
    "            'mask': mask.unsqueeze(0),\n",
    "            'b_min': B_MIN,\n",
    "            'b_max': B_MAX,\n",
    "        }\n",
    "\n",
    "    def eval(self, data, use_octree=False):\n",
    "        '''\n",
    "        Evaluate a data point\n",
    "        :param data: a dict containing at least ['name'], ['image'], ['calib'], ['b_min'] and ['b_max'] tensors.\n",
    "        :return:\n",
    "        '''\n",
    "        with torch.no_grad():\n",
    "            self.netG.eval()\n",
    "            save_path = '%s/%s/result_%s.obj' % (RESULTS_PATH, NAME, data['name'])\n",
    "            gen_mesh(self.netG, self.cuda, data,save_path=save_path, use_octree=use_octree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Core' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-19-108c6f7d7cfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mevaluator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEvaluator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mglob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTEST_FOLDER_PATH\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'*'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_images\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m'png'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;34m'jpg'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[1;34m'mask'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtest_masks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'_mask.png'\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-18-85b11e4e80f6>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, projection_mode)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# create net\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mnetG\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mHGPIFuNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprojection_mode\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcuda\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Using Network: '\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetG\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-17-995da52912d3>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, projection_mode, error_term)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnum_views\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mNUM_VIEWS\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m         \u001b[0mvino_core\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;31m# Replace with OpenVINO model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[1;31m# self.image_filter = HGFilter()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'Core' is not defined"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator()\n",
    "\n",
    "test_images = glob.glob(os.path.join(TEST_FOLDER_PATH, '*'))\n",
    "test_images = [f for f in test_images if ('png' in f or 'jpg' in f) and (not 'mask' in f)]\n",
    "test_masks = [f[:-4]+'_mask.png' for f in test_images]\n",
    "\n",
    "for image_path, mask_path in tqdm.tqdm(zip(test_images, test_masks)):\n",
    "    try:\n",
    "        \"\"\"\n",
    "        輸入準備\n",
    "        \"\"\"\n",
    "        data = evaluator.load_image(image_path, mask_path)\n",
    "        \"\"\"\n",
    "        Inference\n",
    "        \"\"\"\n",
    "        evaluator.eval(data, True)\n",
    "    except Exception as e:\n",
    "        print(\"error:\", e.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a9db727726af8bcc6b2a0c8576d819a831cea777e35791aef72c45bea7213144"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
